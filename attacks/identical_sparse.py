# coding: utf-8
###
 # @file   identical_sparse.py
 #
 #
 # @section DESCRIPTION
 #
 # Collection of attacks which submit f identical gradients, which consist in
 # adding as much of one attack vector to the average of the honest gradients.
 #
 # These attacks have been introduced in/adapted from the following papers:
 # bulyan · El Mhamdi El Mahdi, Guerraoui Rachid, and Rouault Sébastien.
 #          The Hidden Vulnerability of Distributed Learning in Byzantium.
 #          ICML 2018. URL: http://proceedings.mlr.press/v80/mhamdi18a.html
 # empire · Cong Xie, Oluwasanmi Koyejo, Indranil Gupta.
 #          Fall of Empires: Breaking Byzantine-tolerant SGD by Inner Product Manipulation.
 #          UAI 2019. URL: http://auai.org/uai2019/proceedings/papers/83.pdf
 # little · Moran Baruch, Gilad Baruch, Yoav Goldberg.
 #          A Little Is Enough: Circumventing Defenses For Distributed Learning.
 #          2019 Feb 16. ArXiv. URL: https://arxiv.org/pdf/1902.06156v1
 # dissensus 
 #
 # spectral dissensus
###

import tools

import math
import torch
import topology

from . import register

# ---------------------------------------------------------------------------- #
# Generic attack implementation generator. Only difference with identical.py (non sparse) is that it defines 
# the attack reference point as the parameter of the attacked nodes instead of the average of all parameters.

def make_attack(compute_direction):
  """ Make the attack gradient generation closure associated with an attack direction.
  Args:
    compute_direction Attack vector computation, 
    (stacked honest gradients, average honest gradient, forwarded keyword-arguments...) -> attack vector (in the gradient space, no reference)
  Returns:
    Byzantine gradient generation closure
  """
  def attack(
      grad_honests, f_real, f_decl, defense, 
      worker_attacked, communication_step=1, network=None, 
      factor=-16, negative=False, 
      **kwargs
    ):
    """ Generate the attack gradients.
    Args:
      grad_honests          Non-empty list of honest gradients
      f_decl                Number of declared Byzantine gradients
      f_real                Number of Byzantine gradients to generate
      defense               Aggregation rule in use to defeat
      gar                   Aggregation rule used
      worker_attacked       Indice of the attacked worker within grad_honests
      factor                Fixed attack factor if positive, number of evaluations for best attack factor if negative
      negative              Use a negative factor instead of a positive one
      device                Device on which to run the computations, relevant when filterL2 is used as aggregation rule
      ...          Forwarded keyword-arguments
    Returns:
      Generated Byzantine gradients (all references to one)

      Output is such that 
    """
    # Fast path
    if f_real == 0:
      return list()
    
    # Stack and compute the average honest gradient, and then the attack vector
    grad_stck = torch.stack(grad_honests).clone() # There is some inplace modification of grad_stck somewhere
    grad_avg  = grad_stck.mean(dim=0) 

    correct_thetas = [grad_stck[k,:] for k in network.nodes()] # list format for aggregation is required

    # Gradient reference point.
    grad_reference = grad_stck[worker_attacked,:]

    # Gradient attack direction
    grad_att_dir  = compute_direction(grad_stck=grad_stck, grad_avg=grad_avg, worker_attacked=worker_attacked, network=network, **kwargs)
    
    # Point to be far away from (ideal aggregation point for honest nodes) 
    # WARNING : might not be the good one beyond RG-algorithms
    
    weights = torch.tensor(network.weights(worker_attacked), device=grad_stck[0].device)
    grad_diff = grad_stck[worker_attacked,] - grad_stck
    grad_target_honest = grad_stck[worker_attacked,] -  communication_step * (grad_diff * weights[:-f_real, None])

    # Evaluate the best attack factor (if required)
    if factor < 0:
      def eval_factor(factor):
        # evalutation function of the impact of the Byzantine attack scaling
        # Here consists in measuring the distance between the grad_target_honest and the 
        # resulting parameter of the aggregation
        # Apply the given factor
        if negative:
          factor = -factor
        grad_attack = grad_reference + factor * grad_att_dir
        # Measure effective squared distance
        aggregated = defense(
          gradients=(correct_thetas + [grad_attack] * f_real), f=f_decl,
          honest_index=worker_attacked, weights=weights, communication_step=communication_step,
          **kwargs
          )
        
        aggregated.sub(grad_target_honest)

        return aggregated.dot(aggregated).item()
      factor = tools.line_maximize(eval_factor, evals=math.ceil(-factor))
    else:
      if negative:
        factor = -factor
    # Generate the Byzantine gradient from the given/computed factor
    byz_grad = grad_reference + factor * grad_att_dir
    # Return this Byzantine gradient 'f_real' times
    return [byz_grad] * f_real
  # Return the attack closure
  return attack

def check(grad_honests, f_real, defense, worker_attacked, network, factor=-16, negative=False, **kwargs):
  #grad_honests, f_real, defense, factor=-16, negative=False, **kwargs):
  """ Check parameter validity for this attack template.
  Args:
    grad_honests Non-empty list of honest gradients
    f_real       Number of Byzantine gradients to generate
    defense      Aggregation rule in use to defeat
    ...          Ignored keyword-arguments
  Returns:
    Whether the given parameters are valid for this attack
  """
  if not isinstance(grad_honests, list) or len(grad_honests) == 0:
    return f"Expected a non-empty list of honest gradients, got {grad_honests!r}"
  if not isinstance(f_real, int) or f_real < 0:
    return f"Expected a non-negative number of Byzantine gradients to generate, got {f_real!r}"
  if not callable(defense):
    return f"Expected a callable for the aggregation rule, got {defense!r}"
  if not ((isinstance(factor, float) and factor > 0) or (isinstance(factor, int) and factor != 0)):
    return f"Expected a positive number or a negative integer for the attack factor, got {factor!r}"
  if not isinstance(negative, bool):
    return f"Expected a boolean for optional parameter 'negative', got {negative!r}"
  if not isinstance(worker_attacked, int) or worker_attacked < 0 or worker_attacked >= len(grad_honests):
    return f"Expected a node indice (int) for parameter 'worker_attacked', got {worker_attacked!r}"
  if not isinstance(network, topology.CommunicationNetwork):
    return f"Expected a communication network as parameter 'network', got {network!r}"

# ---------------------------------------------------------------------------- #
# Attack vector computations

def bulyan(grad_stck, grad_avg, target_idx=-1, **kwargs):
  """ Compute the attack vector adapted from "The Hidden Vulnerability".
  Args:
    target_idx Index of the targeted coordinate, "all" for all
  See:
    make_attack
  """
  if target_idx == "all":
    return torch.ones_like(grad_avg)
  else:
    assert isinstance(target_idx, int), f"Expected an integer or \"all\" for 'target_idx', got {target_idx!r}"
    grad_att = torch.zeros_like(grad_avg)
    grad_att[target_idx] = 1
    return grad_att

def empire(grad_avg, **kwargs):
  """ Compute the attack vector adapted from "Fall of Empires".
  See:
    make_attack
  """
  return grad_avg

def empire_neg(grad_avg, **kwargs):
  """ Compute the attack vector adapted from "Fall of Empires".
  See:
    make_attack
  """
  return grad_avg.neg()

def little(grad_stck, **kwargs):
  """ Compute the attack vector adapted from "A Little is Enough".
  See:
    make_attack
  """
  return grad_stck.var(dim=0).sqrt_()

def dissensus(grad_stck, worker_attacked, network, **kwargs):
  """
  Compute the attack vector on node worker_attacked from "Clipped Gossip"
  """
  dissensus_vector = torch.tensor(network.laplacian,
               device=grad_stck.device).to(grad_stck.dtype)[worker_attacked, :]
  return torch.matmul(dissensus_vector, grad_stck)

def spectral(grad_stck, worker_attacked, network, **kwargs):
  """
  Compute the attack vector on node worker_attacked from Spectral Heterogeneity 
  """
  fiedler_vector = torch.tensor(network.fiedler_vec, device=grad_stck.device).to(grad_stck.dtype)
  return torch.matmul(fiedler_vector, grad_stck) * fiedler_vector[worker_attacked]

# ---------------------------------------------------------------------------- #
# Attack registrations

# Register the attacks
for name, func in (("sparse_empire", empire), ("sparse_little", little), 
                   ("dissensus", dissensus), ("spectral", spectral)):
   # (("sparse_bulyan", bulyan), ("sparse_empire", empire), ("sparse_empire_neg", empire_neg), 
   # ("sparse_little", little), ("dissensus", dissensus), ("spectral", spectral)):
  register(name, make_attack(func), check)

